{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44imGU9F7fjl"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRqBuENPo5JM"
      },
      "outputs": [],
      "source": [
        "def fetch_all_artists(estilo_id, increment=50, max_retries=3, delay=1):\n",
        "    \"\"\"\n",
        "    Iteratively fetch artist URLs by incrementing the 'ini' parameter.\n",
        "\n",
        "    Parameters:\n",
        "    - base_url (str): The base URL with placeholders for 'ini'.\n",
        "    - increment (int): The amount to increment 'ini' each time.\n",
        "    - max_retries (int): Number of retries for failed requests.\n",
        "    - delay (int or float): Seconds to wait between requests.\n",
        "\n",
        "    Returns:\n",
        "    - List of all artist URLs.\n",
        "    \"\"\"\n",
        "    all_artists = []\n",
        "    ini = 0\n",
        "    session = requests.Session()\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (compatible; Bot/1.0; +https://yourdomain.com/bot)\"\n",
        "    }\n",
        "\n",
        "    while True:\n",
        "        # Construct the URL with the current 'ini' value\n",
        "        params = {\n",
        "            \"ini\": ini,\n",
        "            \"req_pais\": \"ar\",\n",
        "            \"req_estilo\": estilo_id\n",
        "        }\n",
        "        url = f\"https://acordes.lacuerda.net/ARCH/indices.php\"\n",
        "\n",
        "        try:\n",
        "            resp = session.get(url, params=params, headers=headers, timeout=10)\n",
        "            resp.raise_for_status()\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Failed to fetch page with ini={ini}. Error: {e}\")\n",
        "            break  # Stop on request failure\n",
        "\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "        main_ul = soup.find(\"ul\", id=\"i_main\")\n",
        "        if not main_ul:\n",
        "            print(f\"No artist list found on page with ini={ini}. Stopping.\")\n",
        "            break  # No more artists to fetch\n",
        "\n",
        "        # Extract artist links from the current page\n",
        "        artist_links = []\n",
        "        for li in main_ul.find_all(\"li\"):\n",
        "            a_tag = li.find(\"a\")\n",
        "            if a_tag and a_tag.has_attr(\"href\"):\n",
        "                relative_link = a_tag[\"href\"].strip()\n",
        "                full_url = urljoin(\"https://acordes.lacuerda.net\", relative_link)\n",
        "                artist_links.append(full_url)\n",
        "\n",
        "        if not artist_links:\n",
        "            print(f\"No artists found on page with ini={ini}. Stopping.\")\n",
        "            break  # No artists found; assume no more pages\n",
        "\n",
        "        print(f\"Fetched {len(artist_links)} artists from ini={ini}.\")\n",
        "\n",
        "        all_artists.extend(artist_links)\n",
        "\n",
        "        ini += increment\n",
        "        time.sleep(delay)\n",
        "\n",
        "    return all_artists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4-woYLQsIGQ"
      },
      "outputs": [],
      "source": [
        "def extract_urls_from_page(page_url):\n",
        "    \"\"\"\n",
        "    Fetches the HTML content from the given URL and extracts a list of absolute URLs\n",
        "    from the <ul> element with id 'b_main'.\n",
        "\n",
        "    Args:\n",
        "        page_url (str): The URL of the webpage to extract URLs from.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of absolute URLs extracted from the page.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) ' +\n",
        "                      'AppleWebKit/537.36 (KHTML, like Gecko) ' +\n",
        "                      'Chrome/58.0.3029.110 Safari/537.3'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(page_url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()  # Raise HTTPError for bad responses (4XX, 5XX)\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching the URL: {e}\")\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    urls = []\n",
        "\n",
        "    # Find the <ul> element with id 'b_main'\n",
        "    b_main = soup.find('ul', id='b_main')\n",
        "    if not b_main:\n",
        "        print(\"No <ul> with id 'b_main' found.\")\n",
        "        return urls\n",
        "\n",
        "    # Find all <a> tags within this <ul>\n",
        "    a_tags = b_main.find_all('a', href=True)\n",
        "    if not a_tags:\n",
        "        print(\"No <a> tags with href found within <ul id='b_main'>.\")\n",
        "        return urls\n",
        "\n",
        "    for a in a_tags:\n",
        "        href = a['href'].strip()\n",
        "        # Construct the full URL\n",
        "        full_url = urljoin(page_url, href)\n",
        "        urls.append(full_url)\n",
        "\n",
        "    return urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLt9ep_otNgJ"
      },
      "outputs": [],
      "source": [
        "def extract_lyrics_from_url(page_url):\n",
        "    \"\"\"\n",
        "    Fetches the HTML content from the given URL and extracts the lyrics\n",
        "    contained within the <div class=\"rLetra\"> element.\n",
        "\n",
        "    Args:\n",
        "        page_url (str): The URL of the webpage to extract lyrics from.\n",
        "\n",
        "    Returns:\n",
        "        str: The extracted lyrics as a clean string. Returns an empty string\n",
        "             if lyrics are not found or an error occurs.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        'User-Agent': (\n",
        "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
        "            'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
        "            'Chrome/58.0.3029.110 Safari/537.3'\n",
        "        )\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(page_url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()  # Raise HTTPError for bad responses (4XX, 5XX)\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching the URL: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find the <div> with class 'rLetra'\n",
        "    r_letra_div = soup.find('div', class_='rLetra')\n",
        "    if not r_letra_div:\n",
        "        print(\"No <div> with class 'rLetra' found.\")\n",
        "        return \"\"\n",
        "\n",
        "    lyrics = r_letra_div.get_text(separator='<br>', strip=True)\n",
        "\n",
        "    return lyrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phiZjyyL-z35"
      },
      "outputs": [],
      "source": [
        "estilos_id = [\n",
        "    \"bal\",\n",
        "    \"can\",\n",
        "    \"gru\",\n",
        "    \"pop\",\n",
        "    \"rel\",\n",
        "    \"rok\",\n",
        "    \"tra\",\n",
        "    \"tro\",\n",
        "    \"rom\"\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21of_BfS8bgc",
        "outputId": "36fca05c-e8a7-489b-8ba9-c9a43f59ae9d"
      },
      "outputs": [],
      "source": [
        "for estilo in list(set(estilos_id)):\n",
        "  print(estilo)\n",
        "  df_data = list()\n",
        "  artist_urls = fetch_all_artists(\n",
        "    estilo_id=estilo,\n",
        "    increment=50,\n",
        "    max_retries=3,\n",
        "    delay=1)\n",
        "  for artist_url in artist_urls:\n",
        "    songs_urls = extract_urls_from_page(artist_url)\n",
        "    for song_url in songs_urls:\n",
        "      lyrics = extract_lyrics_from_url(song_url)\n",
        "      df_data.append({\"estilo\": estilo, \"artist_url\":artist_url, \"song_url\": song_url, \"lyrics\": lyrics})\n",
        "  df = pd.DataFrame(df_data)\n",
        "  df.to_csv(f\"datasets/{estilo}.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
